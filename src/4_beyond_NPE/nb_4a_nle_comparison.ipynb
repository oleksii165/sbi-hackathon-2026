{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Likelihood Estimation (NLE) vs NPE\n",
    "\n",
    "**Session 4, Part 2** - Hands-on exercise (~30 min)\n",
    "\n",
    "In this notebook, we'll:\n",
    "1. Train **NPE** again on the Lotka-Volterra model as a baseline\n",
    "2. Train **NLE** on the same data\n",
    "3. Run **MCMC** to obtain posterior samples from NLE\n",
    "4. **Compare** the two approaches\n",
    "5. Understand what each estimator learns by **generating synthetic data**\n",
    "6. Validate with **posterior predictive checks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Key Difference\n",
    "\n",
    "| Method | What it learns | How to get posterior |\n",
    "|--------|---------------|---------------------|\n",
    "| **NPE** | $q_\\phi(\\theta|x) \\approx p(\\theta|x)$ | Direct sampling |\n",
    "| **NLE** | $q_\\phi(x|\\theta) \\approx p(x|\\theta)$ | MCMC with $p(\\theta|x) \\propto q_\\phi(x|\\theta) \\cdot p(\\theta)$ |\n",
    "\n",
    "**NLE learns the likelihood**, then uses Bayes' theorem + MCMC to get the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sbi.inference import NPE, NLE\n",
    "from sbi.utils import BoxUniform\n",
    "from sbi.inference import simulate_for_sbi\n",
    "from functools import partial\n",
    "\n",
    "from simulators import (\n",
    "    create_lotka_volterra_prior,\n",
    "    generate_observed_data,\n",
    "    lotka_volterra_simulator,\n",
    "    simulate,\n",
    ")\n",
    "from utils import corner_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Lotka-Volterra with Summary Statistics\n",
    "\n",
    "We'll use **20-dimensional** summary statistics (moments + autocorrelation):\n",
    "\n",
    "**Per population (deer and wolves):**\n",
    "- 5 moments: mean, std, max, skewness, kurtosis\n",
    "- 5 autocorrelation lags: capturing temporal structure\n",
    "\n",
    "This richer representation makes the inference problem more challenging!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use small summary statistics (no autocorrelation)\n",
    "USE_AUTOCORRELATION = True\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Set up prior and observation\n",
    "prior = create_lotka_volterra_prior()\n",
    "x_o, theta_true = generate_observed_data(use_autocorrelation=USE_AUTOCORRELATION)\n",
    "\n",
    "print(f\"Prior bounds: {prior.base_dist.low.numpy()} to {prior.base_dist.high.numpy()}\")\n",
    "print(f\"True parameters: {theta_true.numpy()}\")\n",
    "print(f\"Observation shape: {x_o.shape} (20D summary statistics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Training Data\n",
    "\n",
    "Both NPE and NLE use the same training data: $(\\theta, x)$ pairs from prior $\\times$ simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training data\n",
    "NUM_SIMULATIONS = 20000\n",
    "NUM_WORKERS = 10  # Adapt to your computer resourcers\n",
    "\n",
    "print(f\"Generating {NUM_SIMULATIONS} simulations...\")\n",
    "\n",
    "# Define simulator returning summary stats with auto-correlations.\n",
    "simulator = partial(lotka_volterra_simulator, use_autocorrelation=USE_AUTOCORRELATION)\n",
    "\n",
    "# Use sbi helper function to simulate in parallel\n",
    "theta_train, x_train = simulate_for_sbi(simulator, prior, NUM_SIMULATIONS, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(f\"Training data shapes: theta={theta_train.shape}, x={x_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train NPE (Baseline)\n",
    "\n",
    "First, let's train NPE as our baseline. This should be familiar from Session 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NPE\n",
    "print(\"Training NPE...\")\n",
    "npe_trainer = NPE(prior)\n",
    "npe_trainer.append_simulations(theta_train, x_train)\n",
    "npe_net = npe_trainer.train(training_batch_size=1000)\n",
    "\n",
    "# Build posterior\n",
    "npe_posterior = npe_trainer.build_posterior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from NPE posterior\n",
    "print(\"Sampling from NPE posterior...\")\n",
    "npe_samples = npe_posterior.sample((10000,), x=x_o)\n",
    "print(f\"NPE samples shape: {npe_samples.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train NLE\n",
    "\n",
    "Now let's train NLE. The setup is almost identical to NPE, we just approximate the likelihood this time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train NLE\n",
    "print(\"Training NLE...\")\n",
    "nle_trainer = NLE(prior)\n",
    "nle_trainer.append_simulations(theta_train, x_train)\n",
    "nle_net = nle_trainer.train(training_batch_size=1000);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MCMC Sampling from NLE\n",
    "\n",
    "Here's the key difference: NLE learns the **likelihood** $p(x|\\theta)$, not the posterior.\n",
    "\n",
    "To get posterior samples, we need to run MCMC:\n",
    "\n",
    "$$p(\\theta|x) \\propto p(x|\\theta) \\cdot p(\\theta)$$\n",
    "\n",
    "The `sbi` package handles this for us with `build_posterior(mcmc_method=...)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think First!\n",
    "\n",
    "**Q: Why does NLE require MCMC while NPE doesn't?**\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "- **NPE** directly learns $p(\\theta|x)$ as a normalizing flow, which we can sample from directly\n",
    "- **NLE** learns $p(x|\\theta)$ - to get the posterior, we need to combine it with the prior via Bayes' theorem\n",
    "- The normalizing constant $p(x)$ is intractable, so we use MCMC to sample from the unnormalized posterior\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build NLE posterior with MCMC\n",
    "# Available methods: \"slice_np\", \"slice_np_vectorized\" (default), \"hmc\", \"nuts\"\n",
    "\n",
    "nle_posterior = nle_trainer.build_posterior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Sample from the NLE Posterior\n",
    "\n",
    "Complete the code below to sample from the NLE posterior.\n",
    "\n",
    "**Hint:** The API is the same as NPE, but MCMC will take longer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sample 10,000 samples from the NLE posterior given observation x_o\n",
    "# This will take ~30-60 seconds due to MCMC\n",
    "\n",
    "print(\"Sampling from NLE posterior (MCMC)... this takes a moment.\")\n",
    "\n",
    "nle_samples = # YOUR CODE HERE\n",
    "\n",
    "print(f\"NLE samples shape: {nle_samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solutions_nb_4a_1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Posteriors: NPE vs NLE\n",
    "\n",
    "Let's visualize both posteriors and see how they compare."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantitative comparison: posterior means and standard deviations\n",
    "param_names = [r\"$\\alpha$\", r\"$\\beta$\", r\"$\\delta$\", r\"$\\gamma$\"]\n",
    "\n",
    "print(\"Posterior comparison:\")\n",
    "print(f\"{'Param':<8} {'True':>10} {'NPE mean':>12} {'NPE std':>10} {'NLE mean':>12} {'NLE std':>10}\")\n",
    "print(\"-\" * 64)\n",
    "for i, name in enumerate(param_names):\n",
    "    true_val = theta_true[i].item()\n",
    "    npe_mean = npe_samples[:, i].mean().item()\n",
    "    npe_std = npe_samples[:, i].std().item()\n",
    "    nle_mean = nle_samples[:, i].mean().item()\n",
    "    nle_std = nle_samples[:, i].std().item()\n",
    "    print(f\"{name:<8} {true_val:>10.4f} {npe_mean:>12.4f} {npe_std:>10.4f} {nle_mean:>12.4f} {nle_std:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize posteriors using corner plot\n",
    "prior_limits = [[prior.base_dist.low[i].item(), prior.base_dist.high[i].item()] \n",
    "                for i in range(len(param_names))]\n",
    "\n",
    "fig, axes = corner_plot(\n",
    "    [npe_samples, nle_samples],\n",
    "    labels=[\"NPE\", \"NLE\"],\n",
    "    param_names=param_names,\n",
    "    theta_true=theta_true,\n",
    "    limits=prior_limits,  # comment in to show in prior parameter limits.\n",
    ")\n",
    "plt.suptitle(\"NPE vs NLE Posteriors\", y=1.02, fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save samples for comparison in nb_05 (SNPE notebook)\n",
    "torch.save({\n",
    "    'npe_samples': npe_samples,\n",
    "    'nle_samples': nle_samples,\n",
    "    'theta_true': theta_true,\n",
    "    'x_o': x_o,\n",
    "}, 'npe_nle_samples.pt')\n",
    "print(\"Saved NPE and NLE samples to 'npe_nle_samples.pt'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding What the Estimators Learn\n",
    "\n",
    "Both NPE and NLE train a **conditional density estimator** (a normalizing flow). But what they learn is fundamentally different.\n",
    "\n",
    "### Think First! ðŸ¤”\n",
    "\n",
    "When you **directly sample** from the NPE estimator you get posterior samples.\n",
    "\n",
    "What do you get by sampling from the NLE estimator? \n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answers</summary>\n",
    "\n",
    "- You get samples from the likelihood, i.e., synthetic data **without running the simulator**.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from the NPE **Estimator** directly\n",
    "\n",
    "In `sbi`, the *internal* estimator networks have a distribution-like API, with `.sample` and `log_prob` methods. \n",
    "\n",
    "For example, with NPE, we call `sample` on the estimator network to get posterior samples:\n",
    "\n",
    "```python\n",
    "# NPE: condition on x, sample theta\n",
    "theta_samples = npe_net.sample((num_samples,), condition=x_o)\n",
    "```\n",
    "\n",
    "This is (almost) what `npe_posterior.sample()` does under the hood!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Generate Synthetic Data from the NLE Estimator\n",
    "\n",
    "Now it's your turn! With NLE, we can generate synthetic summary statistics by sampling the trained estimator net conditioned on parameter values.\n",
    "\n",
    "**Your task:** \n",
    "1. Take the true parameters `theta_true` (or any theta from the posterior)\n",
    "2. Use `nle_net.sample()` to generate synthetic summary statistics\n",
    "3. Compare the synthetic data to the real observation `x_o`\n",
    "\n",
    "**Hint:** The API is similar to NPE, but you condition on `theta` instead of `x`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Generate synthetic summary statistics from the NLE estimator\n",
    "# \n",
    "# 1. Use nle_net.sample() with the true parameters\n",
    "# 2. Generate 100 synthetic observations\n",
    "# 3. The condition should be theta_true (shape: [1, 4] for batch dimension)\n",
    "\n",
    "num_synthetic = 1000\n",
    "theta_condition = theta_true.unsqueeze(0)  # Add batch dimension: [4] -> [1, 4]\n",
    "\n",
    "# YOUR CODE HERE: sample synthetic data from the NLE estimator\n",
    "x_synthetic_nle = ???\n",
    "\n",
    "print(f\"Synthetic data shape: {x_synthetic_nle.shape}\")  # Should be [num_synthetic, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load solutions/solutions_nb_4a_2.py\n",
    "\"\"\"Solution for generating synthetic data from NLE estimator\"\"\"\n",
    "\n",
    "# NLE estimator learns p(x|theta), so we condition on theta and sample x\n",
    "# This is the reverse of NPE, where we condition on x and sample theta\n",
    "\n",
    "x_synthetic_nle = nle_net.sample((num_synthetic,), condition=theta_condition).detach().squeeze(1)\n",
    "\n",
    "print(f\"Synthetic data shape: {x_synthetic_nle.shape}\")  # Should be [100, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate ground truth: run the actual simulator with the same theta\n",
    "# This is what NLE should have learned to emulate!\n",
    "\n",
    "x_simulated = lotka_volterra_simulator(\n",
    "    theta_condition.repeat(num_synthetic, 1),  # Repeat theta for num_synthetic runs\n",
    "    use_autocorrelation=USE_AUTOCORRELATION\n",
    ")\n",
    "\n",
    "print(f\"Simulated data shape: {x_simulated.shape}\")\n",
    "print(f\"NLE synthetic shape:  {x_synthetic_nle.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare NLE synthetic data vs actual simulator output\n",
    "# Use prior predictive ranges to show they're actually quite similar\n",
    "stat_names = [\"Mean\", \"Std\", \"Max\", \"Skew\", \"Kurt\"]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "num_bins = \"auto\"\n",
    "\n",
    "for i, name in enumerate(stat_names):\n",
    "    # Get prior predictive range for this statistic\n",
    "    x_min_deer = x_train[:, i].min().item()\n",
    "    x_max_deer = x_train[:, i].max().item()\n",
    "    x_min_wolf = x_train[:, 10+i].min().item()\n",
    "    x_max_wolf = x_train[:, 10+i].max().item()\n",
    "    \n",
    "    # Deer statistics (indices 0-4)\n",
    "    ax = axes[0, i]\n",
    "    ax.hist(x_simulated[:, i], bins=num_bins, alpha=0.8, label='Simulator', color='C0')\n",
    "    ax.hist(x_synthetic_nle[:, i], bins=num_bins, alpha=0.6, label='NLE', color='C1')\n",
    "    ax.hist(x_train[:num_synthetic, i], bins=50, alpha=0.2, label='Prior', color='grey')\n",
    "    ax.set_xlim(x_min_deer, x_max_deer)\n",
    "    ax.set_title(f'Deer {name}')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Count')\n",
    "    if i == 4:\n",
    "        ax.legend(fontsize=8)\n",
    "\n",
    "    # Wolf statistics (indices 10-14)\n",
    "    ax = axes[1, i]\n",
    "    ax.hist(x_simulated[:, 10+i], bins=num_bins, alpha=0.6, color='C0')\n",
    "    ax.hist(x_synthetic_nle[:, 10+i], bins=num_bins, alpha=0.6, color='C1')\n",
    "    ax.hist(x_train[:num_synthetic, i], bins=50, alpha=0.2, label='Prior', color='grey')\n",
    "    ax.set_xlim(x_min_wolf, x_max_wolf)\n",
    "    ax.set_title(f'Wolf {name}')\n",
    "    if i == 0:\n",
    "        ax.set_ylabel('Count')\n",
    "\n",
    "plt.suptitle(f'NLE Synthetic vs Simulator (prior predictive scale, Î¸ = Î¸_true, n={num_synthetic})', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Think First!\n",
    "\n",
    "**Q: We compared NLE's synthetic data to the simulator at the summary statistics level. Why can't we compare them at the raw time series level?**\n",
    "\n",
    "<details>\n",
    "<summary>Click to reveal answer</summary>\n",
    "\n",
    "NLE was trained on **summary statistics**, not raw time series! It learned $p(x|\\theta)$ where $x$ is a 20-dimensional summary vector.\n",
    "\n",
    "Comparing at the time series level would require:\n",
    "1. **Training NLE on raw time series** (2000 time points Ã— 2 species = 4000D) â€” extremely high-dimensional!\n",
    "2. **Modeling complex temporal dynamics** â€” the oscillations, correlations, and patterns are hard to capture with standard density estimators\n",
    "3. **Much more training data** â€” high-dimensional density estimation needs exponentially more samples\n",
    "\n",
    "This is why **summary statistics are so important in SBI**: they compress the data to a manageable dimension while (hopefully) preserving the information needed for inference.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Posterior Predictive Check\n",
    "\n",
    "A crucial validation step: do simulations from posterior samples produce data similar to the observation?\n",
    "\n",
    "We'll generate **synthetic data** from both posteriors and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data from posterior samples\n",
    "n_predictive = 50\n",
    "\n",
    "# Sample parameters from posteriors\n",
    "npe_predictive_theta = npe_samples[:n_predictive]\n",
    "nle_predictive_theta = nle_samples[:n_predictive]\n",
    "\n",
    "# Simulate summary statistics\n",
    "npe_predictive_x = lotka_volterra_simulator(npe_predictive_theta, use_autocorrelation=USE_AUTOCORRELATION)\n",
    "nle_predictive_x = lotka_volterra_simulator(nle_predictive_theta, use_autocorrelation=USE_AUTOCORRELATION)\n",
    "\n",
    "# Simulate time series for visualization\n",
    "time = np.arange(0, 200, 0.1)\n",
    "ts_observed = simulate(theta_true.numpy())\n",
    "ts_npe = [simulate(npe_predictive_theta[i].numpy()) for i in range(n_predictive)]\n",
    "ts_nle = [simulate(nle_predictive_theta[i].numpy()) for i in range(n_predictive)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive: Time series comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# NPE predictive - Prey\n",
    "ax = axes[0, 0]\n",
    "for ts in ts_npe:\n",
    "    ax.plot(time, ts[:, 0], color=\"C0\", alpha=0.2, linewidth=0.5)\n",
    "ax.plot(time, ts_observed[:, 0], color=\"k\", linewidth=2, label=\"observed\")\n",
    "ax.plot([], [], color=\"C0\", alpha=0.5, label=\"NPE predictive\")\n",
    "ax.set_xlabel(\"Time (days)\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"NPE: Prey (Deer)\")\n",
    "ax.legend()\n",
    "\n",
    "# NPE predictive - Predator\n",
    "ax = axes[0, 1]\n",
    "for ts in ts_npe:\n",
    "    ax.plot(time, ts[:, 1], color=\"C0\", alpha=0.2, linewidth=0.5)\n",
    "ax.plot(time, ts_observed[:, 1], color=\"k\", linewidth=2, label=\"observed\")\n",
    "ax.plot([], [], color=\"C0\", alpha=0.5, label=\"NPE predictive\")\n",
    "ax.set_xlabel(\"Time (days)\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"NPE: Predator (Wolf)\")\n",
    "ax.legend()\n",
    "\n",
    "# NLE predictive - Prey\n",
    "ax = axes[1, 0]\n",
    "for ts in ts_nle:\n",
    "    ax.plot(time, ts[:, 0], color=\"C1\", alpha=0.2, linewidth=0.5)\n",
    "ax.plot(time, ts_observed[:, 0], color=\"k\", linewidth=2, label=\"observed\")\n",
    "ax.plot([], [], color=\"C1\", alpha=0.5, label=\"NLE predictive\")\n",
    "ax.set_xlabel(\"Time (days)\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"NLE: Prey (Deer)\")\n",
    "ax.legend()\n",
    "\n",
    "# NLE predictive - Predator\n",
    "ax = axes[1, 1]\n",
    "for ts in ts_nle:\n",
    "    ax.plot(time, ts[:, 1], color=\"C1\", alpha=0.2, linewidth=0.5)\n",
    "ax.plot(time, ts_observed[:, 1], color=\"k\", linewidth=2, label=\"observed\")\n",
    "ax.plot([], [], color=\"C1\", alpha=0.5, label=\"NLE predictive\")\n",
    "ax.set_xlabel(\"Time (days)\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(\"NLE: Predator (Wolf)\")\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle(\"Posterior Predictive Check: Time Series\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive: Summary statistics comparison (showing moments only, not autocorrelation)\n",
    "stat_names = [\"Mean\", \"Std\", \"Max\", \"Skew\", \"Kurt\"]\n",
    "x_obs = x_o.squeeze().numpy()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "positions = np.arange(5)\n",
    "\n",
    "# NPE - Deer stats (moments only: indices 0-4)\n",
    "ax = axes[0, 0]\n",
    "bp = ax.boxplot(npe_predictive_x[:, :5].numpy(), positions=positions, widths=0.6, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('C0')\n",
    "    patch.set_alpha(0.5)\n",
    "ax.scatter(positions, x_obs[:5], color=\"k\", s=100, zorder=10, marker=\"x\", label=\"observed\")\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(stat_names)\n",
    "ax.set_title(\"NPE: Deer Moments\")\n",
    "ax.legend()\n",
    "\n",
    "# NPE - Wolf stats (moments only: indices 10-14)\n",
    "ax = axes[0, 1]\n",
    "bp = ax.boxplot(npe_predictive_x[:, 10:15].numpy(), positions=positions, widths=0.6, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('C0')\n",
    "    patch.set_alpha(0.5)\n",
    "ax.scatter(positions, x_obs[10:15], color=\"k\", s=100, zorder=10, marker=\"x\", label=\"observed\")\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(stat_names)\n",
    "ax.set_title(\"NPE: Wolf Moments\")\n",
    "ax.legend()\n",
    "\n",
    "# NLE - Deer stats (moments only: indices 0-4)\n",
    "ax = axes[1, 0]\n",
    "bp = ax.boxplot(nle_predictive_x[:, :5].numpy(), positions=positions, widths=0.6, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('C1')\n",
    "    patch.set_alpha(0.5)\n",
    "ax.scatter(positions, x_obs[:5], color=\"k\", s=100, zorder=10, marker=\"x\", label=\"observed\")\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(stat_names)\n",
    "ax.set_title(\"NLE: Deer Moments\")\n",
    "ax.legend()\n",
    "\n",
    "# NLE - Wolf stats (moments only: indices 10-14)\n",
    "ax = axes[1, 1]\n",
    "bp = ax.boxplot(nle_predictive_x[:, 10:15].numpy(), positions=positions, widths=0.6, patch_artist=True)\n",
    "for patch in bp['boxes']:\n",
    "    patch.set_facecolor('C1')\n",
    "    patch.set_alpha(0.5)\n",
    "ax.scatter(positions, x_obs[10:15], color=\"k\", s=100, zorder=10, marker=\"x\", label=\"observed\")\n",
    "ax.set_xticks(positions)\n",
    "ax.set_xticklabels(stat_names)\n",
    "ax.set_title(\"NLE: Wolf Moments\")\n",
    "ax.legend()\n",
    "\n",
    "plt.suptitle(\"Posterior Predictive Check: Summary Statistics (moments only)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Discussion: Why Does NLE Win Here?\n",
    "\n",
    "Looking at the results above, **NLE produces tighter, more accurate posteriors** than NPE on this problem. Why?\n",
    "\n",
    "### The Likelihood Has Simpler Structure\n",
    "\n",
    "In this Lotka-Volterra example:\n",
    "- The **posterior is sharply concentrated** in a small region of parameter space\n",
    "- NPE must learn the mapping $x \\to p(\\theta|x)$ for the **entire prior space** (amortized)\n",
    "- With limited simulations (20k), NPE struggles to learn the sharp posterior accurately\n",
    "- NLE learns the **likelihood** $p(x|\\theta)$, which apparently has simpler structure here\n",
    "- MCMC then concentrates the samples in the right region\n",
    "\n",
    "### When Does This Happen?\n",
    "\n",
    "NLE tends to outperform NPE when:\n",
    "1. **Sharp posteriors**: The posterior is concentrated in a small region\n",
    "2. **Simpler likelihood**: The likelihood may be easier to model than the posterior\n",
    "3. **Limited simulation budget**: NPE needs more data to learn accurate posteriors everywhere\n",
    "\n",
    "### NPE Advantages (When They Apply)\n",
    "- **Fast inference**: No MCMC needed once trained\n",
    "- **Fully amortized**: Works instantly for new observations\n",
    "- **Simple workflow**: Train once, sample many times\n",
    "\n",
    "### Can We Help NPE?\n",
    "\n",
    "Yes! **Sequential NPE (SNPE)** addresses this by:\n",
    "- Training in multiple rounds\n",
    "- Focusing simulations on the region where the posterior has mass\n",
    "- No longer fully amortized, but much more simulation-efficient\n",
    "\n",
    "We'll explore this in the next notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Aspect | NPE | NLE |\n",
    "|--------|-----|-----|\n",
    "| **Learns** | $p(\\theta|x)$ directly | $p(x|\\theta)$ |\n",
    "| **Inference** | Direct sampling | MCMC |\n",
    "| **Speed** | Fast (milliseconds) | Slower (seconds-minutes) |\n",
    "| **Prior flexibility** | Fixed at training | Can change |\n",
    "| **Amortization** | Full | Partial |\n",
    "| **Sharp posteriors** | Can struggle | Often better |\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **NPE is not always the best choice** - NLE can outperform NPE when the likelihood has simpler structure\n",
    "2. **Same training data** - Both methods use $(\\theta, x)$ pairs from prior Ã— simulator\n",
    "3. **Different trade-offs** - NPE is fast but may need more simulations; NLE is slower but can be more accurate\n",
    "4. **Always validate** - Posterior predictive checks reveal which method works better for your problem\n",
    "5. **Sequential methods can help** - SNPE focuses simulations where they matter, helping NPE with sharp posteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Next: Sequential Methods\n",
    "\n",
    "We saw that NPE struggled with this problem due to the sharp posterior. In the **next notebook**, we'll explore **Sequential NPE (SNPE)**:\n",
    "\n",
    "- Train NPE in multiple rounds\n",
    "- Each round focuses simulations near the current posterior estimate\n",
    "- Dramatically improves efficiency for sharp posteriors\n",
    "- **Exercise**: Implement SNPE and see how it matches NLE's performance with fewer total simulations!\n",
    "\n",
    "**Continue to Part 3** (slides) for an overview of sequential methods, then hands-on with SNPE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (SBI Hackathon)",
   "language": "python",
   "name": "sbi-hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
